{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COSC2779LabExercises_W7_solution.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNpwy9n0OwHyjmVNiP1c1Si"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FI5KxX96XqtL","colab_type":"text"},"source":["# Lab Exercises - Week: 7\n","**COSC2779 - Deep Learning - 2020**\n","\n","This lab is aimed at understanding how to develop a simple RNN for sentiment classification. During this lab you will:\n","\n","- Encode/Decode text data\n","- Develop a many-to-one RNN model for sentiment classification \n","\n","![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)  This notebook is designed to run on Google Colab. If you like to run this on your local machine, make sure that you have installed TensorFlow version 2.0. "]},{"cell_type":"markdown","metadata":{"id":"CPCHktEmYJFG","colab_type":"text"},"source":["## Setting up the Notebook\n","\n","Let's first load the packages we need."]},{"cell_type":"code","metadata":{"id":"u69WyYrWXbhl","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow_datasets as tfds\n","import pathlib\n","import shutil\n","import tempfile\n","\n","from  IPython import display\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EsG3sfKoYUi3","colab_type":"text"},"source":["We can use the tensor board to view the learning curves. Let's first set it up."]},{"cell_type":"code","metadata":{"id":"QY1sCgDRYRgL","colab_type":"code","colab":{}},"source":["logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n","shutil.rmtree(logdir, ignore_errors=True)\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Open an embedded TensorBoard viewer\n","%tensorboard --logdir {logdir}/models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPnpj881YWCh","colab_type":"text"},"source":["We can also write our own function to plot the models training history ones training has completed."]},{"cell_type":"code","metadata":{"id":"Gu3Rf5FtYZ2R","colab_type":"code","colab":{}},"source":["from itertools import cycle\n","def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 1.0]):\n","  cycol = cycle('bgrcmk')\n","  for name, item in history_hold.items():\n","    y_train = item.history[metric]\n","    y_val = item.history['val_' + metric]\n","    x_train = np.arange(0,len(y_val))\n","\n","    c=next(cycol)\n","\n","    plt.plot(x_train, y_train, c+'-', label=name+'_train')\n","    plt.plot(x_train, y_val, c+'--', label=name+'_val')\n","\n","  plt.legend()\n","  plt.xlim([1, max(plt.xlim())])\n","  plt.ylim(ylim)\n","  plt.xlabel('Epoch')\n","  plt.ylabel(metric)\n","  plt.grid(True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SkpD1XX-YhNs","colab_type":"text"},"source":["## Loading Dataset \n","\n","In this lab we will be using the [IMDB Movie review dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews#imdb_reviewssubwords8k) available in tensoflow datasets. IMDB is a dataset for binary sentiment classification (all the reviews have either a positive or negative sentiment) containing substantially more data than previous benchmark datasets. It provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabelled data for use as well.\n","\n","Let's download the dataset. The version we are downloading below contains a vocabulary of 8K subword. There is another version that contain 32K vocabulary."]},{"cell_type":"code","metadata":{"id":"T3DT8uq0Z1sK","colab_type":"code","colab":{}},"source":["dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n","                          as_supervised=True)\n","train_dataset, test_dataset = dataset['train'], dataset['test']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qXKL4GenaR71","colab_type":"text"},"source":["### Exploring the dataset\n","\n","Let’s explore the dataset to get a better understanding. First, let's print out a random data point in the train set. "]},{"cell_type":"code","metadata":{"id":"dvMWpTpvb8Xn","colab_type":"code","colab":{}},"source":["for x,y in train_dataset.shuffle(100).take(1):\n","  print('The string: \"{}\"'.format(x))\n","  print('The label: {}'.format(y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJsU996Ndvo_","colab_type":"text"},"source":["You can see the text in the dataset is represented by numbers. This is done using an encoder with vocabulary of 8k. The `info` of the dataset provides the encoder that was used for this dataset. \n","\n","Let’s try to use the encoder to print a random datapoint in the train set. "]},{"cell_type":"code","metadata":{"id":"CPSFwq41eXlS","colab_type":"code","colab":{}},"source":["encoder = info.features['text'].encoder\n","print('Vocabulary size: {}'.format(encoder.vocab_size))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3JT0-Upeeif","colab_type":"code","colab":{}},"source":["for x,y in train_dataset.shuffle(100).take(1):\n","  decoded_data = encoder.decode(x)\n","  print('The string: \"{}\"'.format(decoded_data))\n","  print('The label: {}'.format(y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-f5DNOAey-K","colab_type":"text"},"source":["We can also use this encoder to encode any novel sentence."]},{"cell_type":"code","metadata":{"id":"F-vgmLlffUxe","colab_type":"code","colab":{}},"source":["novel_text = \"This is the sixth lab in deep learning course\"\n","\n","encoded_string = encoder.encode(novel_text)\n","print('Encoded string is {}'.format(encoded_string))\n","\n","decoded_string = encoder.decode(encoded_string)\n","print('Encoded string is {}'.format(decoded_string))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v84L0O8wfsyP","colab_type":"text"},"source":["Note that the encoding happens at sub word level. This means that we may get a sequence of numbers that is not equal to the number of words we have."]},{"cell_type":"code","metadata":{"id":"z3gq9652fo1J","colab_type":"code","colab":{}},"source":["for index in encoded_string:\n","  print('{} ----> {}'.format(index, encoder.decode([index])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KV0r0uD7f_dZ","colab_type":"text"},"source":["## Setting up data loader\n","\n","Next create batches of these encoded strings. Use the `padded_batch` method to zero-pad the sequences to the length of the longest string in the batch:"]},{"cell_type":"code","metadata":{"id":"1NKpIhQpgQvL","colab_type":"code","colab":{}},"source":["BUFFER_SIZE = 10000\n","BATCH_SIZE = 64\n","\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n","test_dataset = test_dataset.padded_batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7K1jHh6hmnl","colab_type":"text"},"source":["Note that each batch now has the same length, but not two different batches."]},{"cell_type":"code","metadata":{"id":"g1nyyxtqg2IY","colab_type":"code","colab":{}},"source":["for x,y in train_dataset.take(2):\n","  print(x.shape, y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3pKXkqZziUE5","colab_type":"text"},"source":["## Create a simple model\n","\n","Build a `tf.keras.Sequential` model and start with an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors. we will cover more on embedding layer in lecture 7.\n","\n","A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input—and then to the next. As per our discussions in week 6 lecture lets start with a network with LSTM cells. LSTM allows the network to capture long range dependencies, easily.\n","\n","`return_sequences` determines if we want to predict an output at each time step or not. We only need to predict the output at the final time step as our task is many-to-one. "]},{"cell_type":"code","metadata":{"id":"GvO7Kyx_iTm1","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n","    tf.keras.layers.LSTM(64,  return_sequences=False),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gz-nOZ0pjmYj","colab_type":"text"},"source":["Compile the Keras model to configure the training process:"]},{"cell_type":"code","metadata":{"id":"YDW5qrLCjnDE","colab_type":"code","colab":{}},"source":["model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy', tf.losses.BinaryCrossentropy(from_logits=False, name='BinaryCrossentropy')])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_xUdxYfkGSs","colab_type":"code","colab":{}},"source":["m_histories = {}\n","\n","def get_callbacks(name):\n","  return [\n","    tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=1),\n","  ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mAnzArisj5VV","colab_type":"text"},"source":["### Training the model"]},{"cell_type":"code","metadata":{"id":"sH0uPmOkj4bR","colab_type":"code","colab":{}},"source":["m_histories['simple_model'] = model.fit(train_dataset, epochs=10,\n","                    validation_data=test_dataset, \n","                    validation_steps=30,\n","                    verbose=0, \n","                    callbacks=get_callbacks('models/simple_model'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkVbs9K25WyB","colab_type":"code","colab":{}},"source":["plotter(m_histories, ylim=[0.0, 1.1], metric = 'BinaryCrossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qj9v4FAS3Y2f","colab_type":"text"},"source":["## Create a base model\n","\n","Lets do few modifications to the simple model base on our intuition, in order to get a base model:\n","\n","- The task we have, can benefit from having knowledge about the words in the future as well as the words in the history. The `tf.keras.layers.Bidirectional` wrapper can be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the output. This helps the RNN to learn long range dependencies.\n","\n","- Can use a deep model (with two LSTM layers) to increase the capacity.\n","- Can use a MLP at the output rather than a single layer NN."]},{"cell_type":"code","metadata":{"id":"YYPLJG4b42pV","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy', tf.losses.BinaryCrossentropy(from_logits=False, name='BinaryCrossentropy')])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qa6v_s5H50xD","colab_type":"text"},"source":["### Training the model"]},{"cell_type":"code","metadata":{"id":"CFSfjMdc5K7Q","colab_type":"code","colab":{}},"source":["m_histories['base_model'] = model.fit(train_dataset, epochs=10,\n","                    validation_data=test_dataset, \n","                    validation_steps=30,\n","                    verbose=0, \n","                    callbacks=get_callbacks('models/base_model'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iM2Fdqpf56gu","colab_type":"code","colab":{}},"source":["plotter(m_histories, ylim=[0.0, 1.1], metric = 'BinaryCrossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7EVtUsnv583V","colab_type":"text"},"source":["## Exercises\n","\n","**Use the knowledge you obtained in weeks 1-7 to improve the model**\n","\n","**Things you may try include**\n","\n","- **Regularisation to balance bias vs variance tradeoff**\n","- **GRU vs LSTM**\n","- **Network depth vs feature width**"]}]}