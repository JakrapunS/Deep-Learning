{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COSC2779LabExercises_W10.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMeKglb3HXZHSjKx5bUBOJj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rSj5gstqv9PN"},"source":["---\n","# <div align=\"center\"><font color='blue'>  </font></div>\n","# <div align=\"center\"><font color='blue'> COSC 2779 | Deep Learning  </font></div>\n","## <div align=\"center\"> <font color='blue'> Week 8 Lab Exercises: **Classify text by using transfer learning from a pre-trained embedding**</font></div>\n","---"]},{"cell_type":"markdown","metadata":{"id":"FI5KxX96XqtL"},"source":["# Introduction\n","\n","In this tutorial, you will learn how to classify text by using transfer learning from a pre-trained embedding.\n","\n","A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale language modelling task. You either use the pretrained model as is or use transfer learning to customise this model to a given task.\n","\n","In this tutorial, you will:\n","- Use pre-trained word embeddings in tensorflow.\n","\n","The lab is partly based on [How to use pre-trained word vectors by NormalizedNerd](https://github.com/Suji04/NormalizedNerd/tree/master/Introduction%20to%20NLP)\n","\n","\n","![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)  This notebook is designed to run on Google Colab. If you like to run this on your local machine, make sure that you have installed TensorFlow version 2.0. "]},{"cell_type":"markdown","metadata":{"id":"CPCHktEmYJFG"},"source":["## Setting up the Notebook\n","\n","Let's first load the packages we need."]},{"cell_type":"code","metadata":{"id":"u69WyYrWXbhl"},"source":["import tensorflow as tf\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow_datasets as tfds\n","import pathlib\n","import shutil\n","import tempfile\n","\n","from  IPython import display\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EsG3sfKoYUi3"},"source":["We can use the tensor board to view the learning curves. Let's first set it up."]},{"cell_type":"code","metadata":{"id":"QY1sCgDRYRgL"},"source":["logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n","shutil.rmtree(logdir, ignore_errors=True)\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Open an embedded TensorBoard viewer\n","%tensorboard --logdir {logdir}/models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPnpj881YWCh"},"source":["We can also write our own function to plot the models training history ones training has completed.\n"]},{"cell_type":"code","metadata":{"id":"Gu3Rf5FtYZ2R"},"source":["from itertools import cycle\n","def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 1.0]):\n","  cycol = cycle('bgrcmk')\n","  for name, item in history_hold.items():\n","    y_train = item.history[metric]\n","    y_val = item.history['val_' + metric]\n","    x_train = np.arange(0,len(y_val))\n","\n","    c=next(cycol)\n","\n","    plt.plot(x_train, y_train, c+'-', label=name+'_train')\n","    plt.plot(x_train, y_val, c+'--', label=name+'_val')\n","\n","  plt.legend()\n","  plt.xlim([1, max(plt.xlim())])\n","  plt.ylim(ylim)\n","  plt.xlabel('Epoch')\n","  plt.ylabel(metric)\n","  plt.grid(True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68913yHiLRfO"},"source":["## Loading the dataset\n","\n","We are going to use the HappyDB database for this lab. \n","HappyDB is a collection of happy moments described by individuals experiencing those moments. \n","\n","The task is to classify text of happy moments to classes: \n","\n","- affection\n","- achievement\n","- enjoy_the_moment\n","- bonding\n","- leisure\n","- nature\n","- exercise\n","\n","The data can be downloaded from: [HappyDB](https://megagonlabs.github.io/HappyDB/). I have also uploaded the data to canvas. \n","\n","If you use this dataset for any other purpose, please cite:\n","```\n","@inproceedings{asai2018happydb, \n","  title = {HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments}, \n","  author = {Asai, Akari and Evensen, Sara and Golshan, Behzad and Halevy, Alon\n","  and Li, Vivian and Lopatenko, Andrei and Stepanov, Daniela and Suhara, Yoshihiko\n","  and Tan, Wang-Chiew and Xu, Yinzhan}, \n","  booktitle = {Proceedings of LREC 2018},  \n","  month = {May},   year={2018}, \n","  address = {Miyazaki, Japan}, \n","  publisher = {European Language Resources Association (ELRA)}\n","}\n","```"]},{"cell_type":"markdown","metadata":{"id":"PJrvk_hrYyVa"},"source":["Download data from colab"]},{"cell_type":"code","metadata":{"id":"SWoyfhn3LUPR"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJSvetr-NQhX"},"source":["!cp /content/drive/'My Drive'/COSC2779/COSC2779lab10/cleaned_hm.csv ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTzAwc_FYtfC"},"source":["Read the dataset and explore"]},{"cell_type":"code","metadata":{"id":"wZcS4VoLLrT7"},"source":["data = pd.read_csv(\"cleaned_hm.csv\")\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZbyYcVAZAPw"},"source":["We will be using the `cleaned_hm` as our x and `predicted_category` as our y"]},{"cell_type":"code","metadata":{"id":"MJbmqYdJNfEO"},"source":["data[\"predicted_category\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3kfk_BdNiQv"},"source":["data[\"num_sentence\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34V5vhh-ZBiL"},"source":["## Data cleaning and pre processing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TE5KbANGZvFP"},"source":["### Data pre processing\n","\n","Deleting happy moments with more than 10 sentences"]},{"cell_type":"code","metadata":{"id":"1G_Mr_7mNmN8"},"source":["mod_data = data.loc[data['num_sentence'] <= 10]\n","mod_data[\"predicted_category\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gcX93Nd3ZiFI"},"source":["Categorical to numerical"]},{"cell_type":"code","metadata":{"id":"7A-BMKBYNqUj"},"source":["encode = {\n","    \"affection\" : 0,\n","    \"achievement\"  : 1,       \n","    \"bonding\" : 2,    \n","    \"enjoy_the_moment\" : 3,     \n","    \"leisure\"  : 4,    \n","    \"nature\" : 5,    \n","    \"exercise\" : 6\n","}\n","\n","mod_data[\"predicted_category\"] = mod_data[\"predicted_category\"].apply(lambda x: encode[x])\n","mod_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZBdLvi5Zisz"},"source":["### Text cleaning\n","\n","Cleaning text is an important part in NLP and involves lot of engineering. The code below uses python string package and nltk to do some cleaning.\n","\n","A more popular text pre processing package is RegEx and the associated python library [re](https://docs.python.org/3/library/re.html)"]},{"cell_type":"code","metadata":{"id":"hY42wELnN9V7"},"source":["import string\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","happy_lines = list()\n","lines = mod_data[\"cleaned_hm\"].values.tolist()\n","\n","for line in lines:\n","    # tokenize the text\n","    tokens = word_tokenize(line)\n","\n","    # convert to lower case\n","    tokens = [w.lower() for w in tokens]\n","\n","    # remove puntuations\n","    table = str.maketrans('', '', string.punctuation)\n","    stripped = [w.translate(table) for w in tokens]\n","\n","    # remove non alphabetic characters\n","    words = [word for word in stripped if word.isalpha()]\n","\n","    happy_lines.append(words)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3FFkFBdaZe3"},"source":["The above cleans the `cleaned_hm` column and converts it to list of words. "]},{"cell_type":"code","metadata":{"id":"H6IMewCJaY8A"},"source":["happy_lines[:2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rgylL68Ma0vR"},"source":["### Generating the task vocabulary\n","\n","You can use the keras `Tokenizer` to generate a vocabulary for your data"]},{"cell_type":"code","metadata":{"id":"VQOMXK_AOKLb"},"source":["from tensorflow.python.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","\n","validation_split = 0.20\n","max_length = 55\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(happy_lines)\n","sequences = tokenizer_obj.texts_to_sequences(happy_lines)\n","\n","word_index = tokenizer_obj.word_index\n","print(\"unique tokens - \"+str(len(word_index)))\n","vocab_size = len(tokenizer_obj.word_index) + 1\n","print('vocab_size - '+str(vocab_size))\n","\n","lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n","category =  mod_data['predicted_category'].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fGlkAxIsbTyJ"},"source":["Split the dataset into train and validation"]},{"cell_type":"code","metadata":{"id":"fuYAJre_bSDW"},"source":["indices = np.arange(lines_pad.shape[0])\n","np.random.shuffle(indices)\n","lines_pad = lines_pad[indices]\n","category = category[indices]\n","\n","n_values = np.max(category) + 1\n","Y = np.eye(n_values)[category]\n","\n","num_validation_samples = int(validation_split * lines_pad.shape[0])\n","\n","X_train_pad = lines_pad[:-num_validation_samples]\n","y_train = Y[:-num_validation_samples]\n","X_val_pad = lines_pad[-num_validation_samples:]\n","y_val = Y[-num_validation_samples:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zMou2UUSb19B"},"source":["To demostrate the utility of transfer lerning I will sample a smaller training dataset."]},{"cell_type":"code","metadata":{"id":"2LFto8uhVv_j"},"source":["# Randomly sample some train data\n","train_len = X_train_pad.shape[0]\n","\n","idx = np.random.randint(train_len, size=train_len//25)\n","\n","X_train_pad_sampled = X_train_pad[idx, :]\n","y_train_sampled = y_train[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gbFWe-y6PH0S"},"source":["print('Shape of X_train_pad:', X_train_pad.shape)\n","print('Shape of y_train:', y_train.shape)\n","\n","print('Shape of X_train_pad_sampled:', X_train_pad_sampled.shape)\n","print('Shape of y_train_sampled:', y_train_sampled.shape)\n","\n","print('Shape of X_test_pad:', X_val_pad.shape)\n","print('Shape of y_test:', y_val.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnwp__PFPUEH"},"source":["## No transfer learning\n","\n","First lets try a simple model without transfer learning."]},{"cell_type":"code","metadata":{"id":"SpZjY2R0QeeK"},"source":["def get_callbacks(name):\n","  return [\n","    tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=1),\n","  ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLSAlc-0PP-x"},"source":["from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional\n","from tensorflow.keras.models import Sequential\n","\n","embedding_dim = 100\n","\n","embedding_layer = Embedding(len(word_index) + 1,\n","                            embedding_dim,\n","                            input_length=max_length,\n","                            trainable=True)\n","\n","model_glove = Sequential()\n","model_glove.add(embedding_layer)\n","model_glove.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.25))\n","model_glove.add(Dense(7, activation='softmax'))\n","\n","model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n","\n","print(model_glove.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bj-lrnindORn"},"source":["To save time in the lab, lets train the model for 5 epoch. You may change this later."]},{"cell_type":"code","metadata":{"id":"BOhaS09ldOkF"},"source":["EPOCH = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTQbJo5mQGPG"},"source":["m_histories = {}\n","m_histories['no_TL'] = model_glove.fit(X_train_pad_sampled, y_train_sampled, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val), callbacks=get_callbacks('models/no_TL'), verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcvUlhA-XlFt"},"source":["plotter(m_histories, ylim=[0.0, 2.0], metric = 'loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SyvQusPmQ--3"},"source":["plotter(m_histories, ylim=[0.0, 1.1], metric = 'categorical_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSGobNhue6CN"},"source":["**What are your observations?**"]},{"cell_type":"markdown","metadata":{"id":"CXbzuyW3XzOE"},"source":["## With Transfer Learning\n","\n","Now lets explore how we can transfere the word embeddings. We will be using the GloVe word embeddings made avaialbe stanford reserchers: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n","\n","I have downloaded the 100 dimentional embedding matrix trained on wikipedea. You can download the appropriate word embedding from the above site and upload to your google drive."]},{"cell_type":"code","metadata":{"id":"B_lC102uTenE"},"source":["!cp /content/drive/'My Drive'/COSC2779/COSC2779lab10/glove.6B.100d.txt ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lgavdpTfENa"},"source":["Read the GloVe vectors from file"]},{"cell_type":"code","metadata":{"id":"qsCX-Fr4YHPb"},"source":["file = open('glove.6B.100d.txt', encoding='utf-8')\n","\n","glove_vectors = dict()\n","for line in file:\n","  values = line.split()\n","  word = values[0]\n","  features = np.asarray(values[1:])\n","  glove_vectors[word] = features\n","\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3csxlcdfNW5"},"source":["Transfere the GloVe embedding vectors to your embedding matrix. This will be done by mapping our task vocabulary to the GloVe vocabulary."]},{"cell_type":"code","metadata":{"id":"MdWjv19JYK3T"},"source":["E_T = np.zeros((len(word_index) + 1, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = glove_vectors.get(word)\n","    if embedding_vector is not None:\n","        E_T[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRzeS8uNftr-"},"source":["Train the model. Note the chaanges to the embedding layer."]},{"cell_type":"code","metadata":{"id":"tKW-QANuXvpT"},"source":["from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional\n","from tensorflow.keras.models import Sequential\n","\n","embedding_layer_TL = Embedding(len(word_index) + 1,\n","                            embedding_dim,\n","                            weights=[E_T],\n","                            input_length=max_length,\n","                            trainable=False)\n","\n","model_glove = Sequential()\n","model_glove.add(embedding_layer_TL)\n","model_glove.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.25))\n","model_glove.add(Dense(7, activation='softmax'))\n","\n","model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n","\n","print(model_glove.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbbEMxO1YiPI"},"source":["m_histories['with_TL'] = model_glove.fit(X_train_pad_sampled, y_train_sampled, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val), callbacks=get_callbacks('models/with_TL'), verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfm81NKVYx67"},"source":["plotter(m_histories, ylim=[0.0, 2], metric = 'loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLkHKw_qYx7B"},"source":["plotter(m_histories, ylim=[0.0, 1.1], metric = 'categorical_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJupRm8TgCK2"},"source":["**Did you see an improvement when using TL**"]},{"cell_type":"code","metadata":{"id":"H7vjDTk_bzdX"},"source":["import os\n","from tensorboard.plugins import projector\n","\n","# Save Labels separately on a line-by-line manner.\n","with open(os.path.join(logdir, 'metadata.tsv'), \"w\") as f:\n","  for word, i in word_index.items():\n","    f.write(\"{}\\n\".format(word))\n","\n","\n","# Save the weights we want to analyse as a variable. Note that the first\n","# value represents any unknown word, which is not in the metadata, so\n","# we will remove that value.\n","weights = tf.Variable(model_glove.layers[0].get_weights()[0][1:])\n","# Create a checkpoint from embedding, the filename and key are\n","# name of the tensor.\n","checkpoint = tf.train.Checkpoint(embedding=weights)\n","checkpoint.save(os.path.join(logdir, \"embedding.ckpt\"))\n","\n","# Set up config\n","config = projector.ProjectorConfig()\n","embedding = config.embeddings.add()\n","# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n","embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n","embedding.metadata_path = 'metadata.tsv'\n","projector.visualize_embeddings(logdir, config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i5mJxzlGZ28q"},"source":["for layer in model_glove.layers:\n","  print(layer.name)\n","  if layer.name == 'embedding_1':\n","    layer.trainable = True"],"execution_count":null,"outputs":[]}]}