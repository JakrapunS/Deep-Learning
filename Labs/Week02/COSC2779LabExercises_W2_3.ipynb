{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of COSC2779LabExercises_W2_3.ipynb","provenance":[{"file_id":"1i0-Q3RtpI8X_ORL4SXXxDlbWyTbbeCce","timestamp":1594177954724}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMcU3H2gcspuX7mlrtoDujk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3g1TtWC5VcxV"},"source":["---\n","# <div align=\"center\"><font color='blue'>  </font></div>\n","# <div align=\"center\"><font color='blue'> COSC 2779 | Deep Learning  </font></div>\n","## <div align=\"center\"> <font color='blue'> Week 1-2 Lab Exercises: **Introduction to Tensorflow**</font></div>\n","---"]},{"cell_type":"markdown","metadata":{"id":"9bNp_WBWASqp"},"source":["\n","\n","# introduction\n","\n","This lab is aimed at introducing the fundamentals of TensorFlow 2.0. The Lab is organized into six sub modules: \n","1. Minimal example:  2D Linear regression. \n","2. Exercise: Classifying hand written text MNIST. \n","3. Advanced TensorFlow example: Understanding gradient tape and writing own training loop \n","4. Saving models and checkpointing \n","5. Working with TensorBoard \n","6. Exercise: Putting everythin together - FasionMNIST\n","\n","**The lab assumes that you are familiar with Google Colab**. Please complete ``Week 01 self-study lab: Introduction to Google Colab`` before attempting this lab. \n","\n","![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)  This notebook is designed to run on Google Colab. If you like to run this on your local machine, make sure that you have installed TensorFlow version 2.0. \n","\n","(*most online tutorials still contain TensorFlow 1.x code, therefore be careful when using online resources - if you see any reference to sessions then most probably that is TensorFlow 1.x code*) \n","\n","To start using TensorFlow lets load it and check the version."]},{"cell_type":"code","metadata":{"id":"tYjG4PdBAQUQ"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# print the tensorflow version\n","print(\"Tensorflow version is: \", tf.__version__)\n","\n","# produce an error if version is not equal to 2\n","assert tf.__version__[0] == '2'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQj4YCGPE_0Q"},"source":["## Minimal example:  2D Linear regression\n","\n","We will start with a simple example. In this example the task is to fit a linear model to a set of synthetic data. We assume 2D input attributes \\\\( \\mathbf{x} \\in \\mathbb{R}^2\\\\). The synthetic data is generated using the following equation. \n","\n","\\\\( y = 2 + 1.5 x_1 + 3.5 x_2 + noise\\\\) \\\\\n","where the noise is Guassian with 0 mean and standard deviation of 0.3.\n"]},{"cell_type":"code","metadata":{"id":"etHrN0KPFwlT"},"source":["N = 1000    # Number of data points in each set\n","w = [2.0, 1.5, 3.5]   # True weight vector (This defines our unknown target function)\n","\n","# Lets generate some traing data\n","x1 = np.random.uniform(low=-10, high=10, size=(N, 1))\n","x2 = np.random.uniform(low=-10, high=10, size=(N, 1))\n","\n","noise = np.random.normal(loc=0.0, scale=0.3, size=(N, 1))  # Generate some Gaussian noise\n","y = w[0] + w[1]*x1 + w[2]*x2 + noise    # Unkown target function\n","\n","train_data_attibutes = np.hstack((x1, x2))\n","train_data_target = y\n","\n","# Now lets generate some i.i.d test data\n","x1 = np.random.uniform(low=-10, high=10, size=(N, 1))\n","x2 = np.random.uniform(low=-10, high=10, size=(N, 1))\n","\n","noise = np.random.normal(loc=0.0, scale=1.0, size=(N, 1))  # Generate some Gaussian noise\n","y = w[0] + w[1]*x1 + w[2]*x2 + noise    # Unkown target function\n","\n","test_data_attibutes = np.hstack((x1, x2))\n","test_data_target = y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZxwH87vvsyW"},"source":["Lets visualise the data. Since we have a very simple datast, we can actually plot it using the scatter function."]},{"cell_type":"code","metadata":{"id":"MfroKEKAIS-5"},"source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.scatter(train_data_attibutes[:, 0], train_data_attibutes[:, 1], train_data_target, c='r', marker='o')\n","\n","ax.set_xlabel('x1 Label')\n","ax.set_ylabel('x2 Label')\n","ax.set_zlabel('y Label')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PKbCDfdzpUic"},"source":["As we discussed in the first lecture, most machine learning algorithms can be described with a fairly simple recipe of four ingredients. Lets define the four ingredients of this problem.\n","\n","1.   Dataset: Generated in the previous section\n","2.   Cost function: Mean squared error \n","3.   Model: Linear regression\n","4.   Optimisation procedure: Stochastic gradient decent\n","\n","Defining these components in TensorFlow (using some help from Keras) is quite straight forward. \n","Lets see how this is done.\n","\n","As you will descover, there are a many ways in TensorFlow to build models and train them. TensorFlow has many APIs that you can use, and trying to use all these APIs can be confusing.\n","\n","Lets build a simple one unit linear perceptron using the Keras Sequential API. In keras Sequential API you have to provide a list of NN layers in the order they appear in the network from input to output. In this case we have only one layer. "]},{"cell_type":"code","metadata":{"id":"hSWGKanLI3MV"},"source":["output_dim = 1\n","\n","mse = tf.keras.losses.MeanSquaredError() # Cost function\n","model = tf.keras.Sequential([tf.keras.layers.Dense(output_dim, use_bias=True, input_shape=(2,))]) # model one linear perceptron\n","custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001) # optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skiqmD-08Zu1"},"source":["We now have all the ingredients. Lets compile (Configures the model for training) the model and train it.\n","\n","The compile command generate the computation graph. `model.fit()` conduct the optimisation to find the optimal weights. Note that 30\\% data is used as the validation set.\n","\n","The training is run for 100 epoch. One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network ONCE."]},{"cell_type":"code","metadata":{"id":"tnaI3Z4j8YtK"},"source":["model.compile(optimizer=custom_optimizer, loss=mse) # attach the cost function and optimizer to the model\n","\n","H = model.fit(train_data_attibutes, train_data_target, validation_split=0.3, epochs=100, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wE8zerq9TbW"},"source":["**Try changing the verbose value to 1, 2 and observe the output**\n","\n","Now we can extract the optimal weights learned by our model and compare them to the original function (using which the synthetic data was generated)"]},{"cell_type":"code","metadata":{"id":"sKDE-OaPLfg-"},"source":["weights = model.layers[0].get_weights()[0]\n","bias = model.layers[0].get_weights()[1]\n","\n","print(\"Weights of the unknown target function: \", w)\n","print(\"Weights estimated: \", bias.round(1).tolist() + weights.round(1).reshape((-1,)).tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pu_t1yOW_BO8"},"source":["The history of the trained model object (H) can be used to plot the learning curve."]},{"cell_type":"code","metadata":{"id":"NDiJGCHtW1K6"},"source":["plt.style.use(\"ggplot\")\n","plt.figure(figsize=(10,5))\n","Nepoch=100\n","plt.subplot(1,2,1)\n","plt.plot(np.arange(0, Nepoch), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, Nepoch), H.history[\"val_loss\"], label=\"validation_loss\")\n","plt.title(\"Training Loss on Dataset\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss\")\n","plt.legend(loc=\"upper right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WPlCjzsU_xeh"},"source":["We can now predict the target variable for the test set and visualise the results."]},{"cell_type":"code","metadata":{"id":"9M0g51IzZb1P"},"source":["y_hat = model.predict(test_data_attibutes).round(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eshRMHW1Z0LD"},"source":["plt.scatter(test_data_target, y_hat)\n","plt.xlabel('Outputs')\n","plt.ylabel('Targets')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gcLeaEnf1HgE"},"source":["**What can you say about the learned model?**"]},{"cell_type":"markdown","metadata":{"id":"DlpqptPjylQj"},"source":["## Exercise: Classifying hand written text classification MNIST. \n","\n","In this exercise you will use the famous MNIST dataset to develop a simple MLP model to recognise hand writtend digits. The MNIST dataset is included in TensorFlow datasets. TensorFlow Datasets is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks. More information is available via [link text](https://www.tensorflow.org/datasets)\n","\n","(optional step) before we start this section, lets restart the kernel to remove any variables that we have created. This will give us a clean slate and does not have to worry about getting confused with the variables and code we have written in the previous section.\n","\n","You can do this by either clicking  Runtime -> Restart runtime in file menu of the notebook or by running the following code. "]},{"cell_type":"code","metadata":{"id":"3WoFnPlEqMMw"},"source":["# import os\n","# os._exit(00)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqJg-AX_BPSu"},"source":["Loading the MNIST dataset and normalising the pixel values"]},{"cell_type":"code","metadata":{"id":"yUzzbPZEqzpo"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfVOrJCTBZfn"},"source":["Visualise few instances of the dataset"]},{"cell_type":"code","metadata":{"id":"PzdLlY8YrAiE"},"source":["plt.figure(figsize=(12,4))\n","\n","for i, image in enumerate(x_train[:3]):\n","    plt.subplot(1,3,i+1)\n","    plt.imshow(image)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8Oegrc3sDH3"},"source":["In this exercise you have to build a simple one hidden layer MLP using the Keras Sequential API.\n","Â \n","The model should consist of:\n","1. Layer to convert \\\\([28,28]\\\\) matrix input to a vector of \\\\(28\\times28\\\\)\n","2. Hidden layer with 128 neurones and `relu` activation\n","3. Dropout regularisation\n","4. output layer with 10 units and softmax activation\n","\n","Use `adam` optimiser and `categorical cross-entropy` loss.\n","\n","<font color='red'>**TODO:** Build your model in the cell below</font>"]},{"cell_type":"code","metadata":{"id":"9qTCtDi4r0Zj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0XBAPc_C7Tm"},"source":["<font color='red'>**TODO:** Train the model for 5 epoch and evaluate the performance on the test set. </font>"]},{"cell_type":"code","metadata":{"id":"7fZYMv3Sr7Ds"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9EH4LoT0sYh4"},"source":["- **How can you add ridge regularisation penalty to the hidden layer?**\n","- **What is the generalisation gap for the model you trained?**\n","- **What are the hyper parameters of the above model?**\n","- **How would you select the values of the hyper parameters?**"]},{"cell_type":"markdown","metadata":{"id":"T_i4TdTpy7jh"},"source":["## Advanced TensorFlow example: Understanding gradient tape and writing own training loop \n","\n","Now, we will explore a more advanced usage of TensorFlow. Following API will provide more flexibility and can be used when you need to customize yor models. \n","Note that you can specify more unique architectures with the forward pass. We will use the same MNIST dataset.\n","\n","Before we start with the ML examples. Lets explore what Automatic differentiation and gradient tape is. More information at [Tensorflow Tutorials](https://www.tensorflow.org/guide/autodiff)\n","\n","Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks. To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.\n","\n","TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variables`. TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\n","\n","Here is a simple example that calculate gradient at x=2:\n","\n","\\\\( \\frac{\\partial \\left( 3x^2 + 2x\\right )}{\\partial x} \\\\)"]},{"cell_type":"code","metadata":{"id":"DTH-scgEuNQG"},"source":["x = tf.Variable(2.0)\n","with tf.GradientTape() as tape:\n","  y = 3 * x * x + 2*x\n","dy_dx = tape.gradient(y, x) # Will compute gradient\n","print(dy_dx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4HxJW3BP8Yf7"},"source":["Was the answer as expected?\n","\n","Lets now load the MNIST dataset (This time using a different API)"]},{"cell_type":"code","metadata":{"id":"sjxBCMFWs4Ih"},"source":["import tensorflow_datasets as tfds\n","\n","# Function to Preprocess the dataset \n","def convert_types(image, label):\n","  image = tf.cast(image, tf.float32)\n","  image /= 255\n","  return image, label\n","\n","dataset, info = tfds.load('mnist', data_dir='gs://tfds-data/datasets', with_info=True, as_supervised=True)\n","mnist_train, mnist_test = dataset['train'], dataset['test']\n","\n","# Do normalization and prepare batches\n","batch_size = 32\n","mnist_train = mnist_train.map(convert_types).shuffle(10000).batch(batch_size)\n","mnist_test = mnist_test.map(convert_types).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0bfG1Sbx-wjX"},"source":["**What does shuffle do in the above code block?** More information at [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)"]},{"cell_type":"code","metadata":{"id":"SEz76xLZ83bH"},"source":["# Print the shapes of the first 3 batches\n","for image, label in mnist_train.take(3):  # example is (image, label)\n","  print(image.shape, label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ScAxd5C_VG_"},"source":["Now lets instantiate a Model by subclassing the Keras Model class, in that case, you should define your layers in `__init__` and you should implement the model's forward pass in `call`.\n","\n","The model is same as the last exercise:\n","1. Layer to convert \\\\([28,28]\\\\) matrix input to a vector of \\\\(28\\times28\\\\)\n","2. Hidden layer with 128 neurones and `relu` activation\n","3. Dropout regularisation\n","4. output layer with 10 units and softmax activation"]},{"cell_type":"code","metadata":{"id":"W_i2YPzutTTA"},"source":["from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout\n","from tensorflow import keras\n","from tensorflow.keras import Model\n","\n","class MNISTModel(Model):\n","  def __init__(self):\n","    super(MNISTModel, self).__init__()\n","    self.flatten = Flatten(input_shape=(28, 28))\n","    self.d1 = Dense(128, activation='relu')\n","    self.d2 = Dense(10, activation='softmax')\n","    self.drop = Dropout(0.2)\n","\n","  def call(self, x):\n","    x = self.flatten(x)\n","    x = self.d1(x)\n","    x = self.drop(x)\n","    return self.d2(x)\n","\n","model = MNISTModel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kfPDBjl-BGak"},"source":["We can now define the optimiser, cost function and the evaluation measures. "]},{"cell_type":"code","metadata":{"id":"lEsN6pXxtx61"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWpOAHGdBfMj"},"source":["Next, lets write a function to train the model. Note the `tf.function` annotation on the function below. This means that this function will be compiled into a graph in the backend, allowing it to run efficiently as TensorFlow can optimize the function for you. This automatic conversion of Python code to its graph representation is called AutoGraph, and this creates callable graphs from Python functions."]},{"cell_type":"code","metadata":{"id":"1rxO84vjt0Sb"},"source":["@tf.function\n","def train_step(image, label):\n","  with tf.GradientTape() as tape:\n","    predictions = model(image)\n","    loss = loss_object(label, predictions)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(label, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KXHYf7PzuORs"},"source":["Next define the function for testing. Note that we do not have to compute the gradients while testing. "]},{"cell_type":"code","metadata":{"id":"ApKP80oiueix"},"source":["@tf.function\n","def test_step(image, label):\n","  predictions = model(image)\n","  t_loss = loss_object(label, predictions)\n","\n","  test_loss(t_loss)\n","  test_accuracy(label, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UgPNtdOoCSyi"},"source":["We now have all the components. lets train the model for 5 epoch."]},{"cell_type":"code","metadata":{"id":"8cVka9DvukPX"},"source":["EPOCHS = 5\n","\n","for epoch in range(EPOCHS):\n","  print(\"starting Epoch: \", epoch)\n","  # iterate over all batches in the dataset\n","  for image, label in mnist_train:\n","    train_step(image, label)\n","\n","  for test_image, test_label in mnist_test:\n","    test_step(test_image, test_label)\n","\n","  print('Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'.format(epoch+1,\n","                         train_loss.result(),\n","                         train_accuracy.result()*100,\n","                         test_loss.result(),\n","                         test_accuracy.result()*100))\n","  \n","  train_loss.reset_states()\n","  test_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_accuracy.reset_states()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekFUBgY_Dw8b"},"source":["<font color='red'>**TODO:** Change the model to have a 3x3 convolution layer with 64 channels as the first layer. Add l2 regularisation to the convolution layer.</font>"]},{"cell_type":"markdown","metadata":{"id":"jxEBXqgWzFkr"},"source":["## Saving models and checkpointing\n","\n","The description of checkpointing from [TensorFlow Documentation](https://www.tensorflow.org/guide/checkpoint)\n","\n","The phrase \"Saving a TensorFlow model\" typically means one of two things:\n","1. Checkpoints\n","2. SavedModel\n","\n","Checkpoints capture the exact value of all parameters (tf.Variable objects) used by a model. Checkpoints do not contain any description of the computation defined by the model and thus are typically only useful when source code that will use the saved parameter values is available.\n","\n","The SavedModel format on the other hand includes a serialized description of the computation defined by the model in addition to the parameter values (checkpoint). Models in this format are independent of the source code that created the model. They are thus suitable for deployment via TensorFlow Serving, TensorFlow Lite, TensorFlow.js, or programs in other programming languages (the C, C++, Java, Go, Rust, C# etc. TensorFlow APIs).\n","\n","(optional step) before we start this section, lets restart the kernel to remove any variables that we have created. This will give us a clean slate and does not have to worry about getting confused with the variables and code we have written in the previous section.\n","\n","You can do this by either clicking  Runtime -> Restart runtime in file menu of the notebook or by running the following code. "]},{"cell_type":"code","metadata":{"id":"KYz-Q7UAH9xh"},"source":["# import os\n","# os._exit(00)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pfhhJX6cLFYR"},"source":["First lets check how the savemodel works. We need to define a model first. Same code we used in previous sections."]},{"cell_type":"code","metadata":{"id":"WptkXfdsG9Sb"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# load data\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","# Create model\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compile and train\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","\n","# Print test accuracy\n","test_loss, test_acc = model.evaluate(x_test, y_test)\n","print('Test accuracy:', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OijqPrSeLVAC"},"source":["Now we have a trained model. Lets save the model to file."]},{"cell_type":"code","metadata":{"id":"ZLS_eWklIINV"},"source":["model.save('my_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gjcf-3ALaIG"},"source":["Check the local directory to see if the model is saved and you can see file `my_model.h5`\n","\n","Next load the saved model and evaluate it. Note that this could happen in a different script on different machine. \n","\n","**NOTE: savemodel does not work with the models defined using functional API (Advanced model above) you need to use save_weights**. For detaild example of how to save model for such models go to [TensorFlow Documentation](https://www.tensorflow.org/guide/checkpoint) \n","\n","The `summary()` function prints out the model structure and is a good tool to debug NN."]},{"cell_type":"code","metadata":{"id":"hfzRjBCNILfH"},"source":["new_model = tf.keras.models.load_model('my_model.h5')\n","new_model.summary()\n","\n","print('\\n\\n')\n","test_loss, test_acc = new_model.evaluate(x_test, y_test)\n","print('\\n\\nTest accuracy:', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EpltxtfNMSCY"},"source":["Checkpoints allow you to save the intermediate states of your model while trining. \n","\n","**It is highly recommended that you use checkpointing when doing the assignments**. Checkpoints will allow you to start from the last state if something happens to your job mid training and can save time. \n","\n","The checkpoint files will disapear if colab session is destroyed. **Write code to move the checkpoints to the google drive - use last weeks lab meterial**"]},{"cell_type":"code","metadata":{"id":"WIzBG5oKI0Lp"},"source":["import os\n","checkpoint_path = \"training_1/cp.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create checkpoint callback\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 verbose=1)\n","\n","model.fit(x_train, y_train, epochs=5, verbose=2, callbacks = [cp_callback])  # pass callback to training"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwQS9g0hN3o_"},"source":["More information on `callbacks.ModelCheckpoint` is at [link text](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)\n","\n","**Go through the documentation and find how you can change the code to**\n","1. **Only keep the model that has achieved the \"best performance\" so far**\n","2. **Save the model at the end of every epoch regardless of performance.**\n","\n","**Note the epoch 1 accuracy is high for this model. why?**\n","\n","check the latest checkpoint"]},{"cell_type":"code","metadata":{"id":"634gLOHQNhVM"},"source":["latest = tf.train.latest_checkpoint(checkpoint_dir)\n","latest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cnSZLNt_N9JS"},"source":["Load the weights from the latest checkpoint and evalaute the model"]},{"cell_type":"code","metadata":{"id":"cUuCCO83N8Un"},"source":["model.load_weights(latest)\n","# Print test accuracy\n","test_loss, test_acc = model.evaluate(x_test, y_test)\n","print('Test accuracy:', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"siEZQJyMJW70"},"source":["## Working with TensorBoard \n","\n","TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, and much more.\n","\n","This section will show how to quickly get started with TensorBoard and use it in notebooks.\n","\n","(optional step) before we start this section, lets restart the kernel to remove any variables that we have created. This will give us a clean slate and does not have to worry about getting confused with the variables and code we have written in the previous section.\n","\n","You can do this by either clicking  Runtime -> Restart runtime in file menu of the notebook or by running the following code. "]},{"cell_type":"code","metadata":{"id":"JQttzpwyPclS"},"source":["# import os\n","# os._exit(00)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6LrTP-iQsNd"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yo5wiFu1YxfB"},"source":["Create a model and compile it."]},{"cell_type":"code","metadata":{"id":"WgI2g8WUPp8y"},"source":["import tensorflow as tf\n","import datetime, os\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# load data\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","# Create model\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compile and train\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrOhOpGRY-Qb"},"source":["When training with Keras `Model.fit()`, adding the `tf.keras.callbacks`.\n","\n","TensorBoard callback ensures that logs are created and stored. Place the logs in a timestamped subdirectory to allow easy selection of different training runs."]},{"cell_type":"code","metadata":{"id":"buO49eIlY--D"},"source":["logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUNOy_lXZfE8"},"source":["Start TensorBoard in notebook."]},{"cell_type":"code","metadata":{"id":"28OBIq2hRHNC"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NafiyYNXZzvQ"},"source":["Now start trining the model. You will see the tensorboard in the above block get updated while the training is running. "]},{"cell_type":"code","metadata":{"id":"_vp0vPS_Rfqn"},"source":["model.fit(x=x_train, \n","            y=y_train, \n","            epochs=50, \n","            verbose=0,\n","            validation_data=(x_test, y_test), \n","            callbacks=[tensorboard_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l-ul62GwlzGk"},"source":["## Exercise: Putting everythin together - FasionMNIST\n","Lets practice what we have learned so far.\n","\n","For this Exercise you will use the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. \n","\n","Develop a classification model to classify images in the dataset using tensorflow. The code should include checkpointing and tensorboard. \n","\n","Download the FashionMNIST dataset"]},{"cell_type":"code","metadata":{"id":"VjLfJFzGmNZ8"},"source":["import tensorflow_datasets as tfds\n","\n","dataset, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True)\n","fmnist_train, fmnist_test = dataset['train'], dataset['test']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_f442KBMpPyL"},"source":["Visualise few images. "]},{"cell_type":"code","metadata":{"id":"NMJvqraFnvr1"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.figure(figsize=(16,4))\n","i=1\n","for image, label in fmnist_train.take(4):  # example is (image, label)\n","  plt.subplot(1,4,i)\n","  plt.imshow(np.squeeze(image))\n","  i = i+1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWZ_4kbTpjVt"},"source":["<font color='red'>**TODO:** Complete the Code.</font>"]},{"cell_type":"code","metadata":{"id":"6FxOYyeMdVby"},"source":[""],"execution_count":null,"outputs":[]}]}