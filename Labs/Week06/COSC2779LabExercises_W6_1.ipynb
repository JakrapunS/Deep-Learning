{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COSC2779LabExercises_W6_1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMpnujnjnb0qFQ/F/G3uOpK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ucwyAnHSWTUo"},"source":["---\n","# <div align=\"center\"><font color='blue'>  </font></div>\n","# <div align=\"center\"><font color='blue'> COSC 2779 | Deep Learning  </font></div>\n","## <div align=\"center\"> <font color='blue'> Week 6 Lab Exercises: **Practical methodology**</font></div>\n","---"]},{"cell_type":"markdown","metadata":{"id":"2YtVkZDCH0kn"},"source":["# Introduction\n","\n","This lab is aimed at understanding how to develop a CNN from scratch. Will be based on solveing CIFAR 10 task. During this lab you will:\n","\n","- Learn how to establish a base model.\n","- Debug the base model and setup instrumentation.\n","- Do manual hyperparameter tuning.\n","\n","\n","![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)  This notebook is designed to run on Google Colab. If you like to run this on your local machine, make sure that you have installed TensorFlow version 2.0. "]},{"cell_type":"code","metadata":{"id":"Kh2AVT0LwbSX"},"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n","\n","import tensorflow as tf\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow_datasets as tfds\n","import pathlib\n","import shutil\n","import tempfile\n","\n","from  IPython import display\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1TVJBjn6zYa"},"source":["## Setting up Instrumentation\n","\n","We can use the tensor board to view the learning curves, activation and weight hostograms. Lets first set it up."]},{"cell_type":"code","metadata":{"id":"rN5-iJ44HhrP"},"source":["logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n","shutil.rmtree(logdir, ignore_errors=True)\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Open an embedded TensorBoard viewer\n","%tensorboard --logdir {logdir}/models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOXH1fDTHsuh"},"source":["We can also write our own function to plot the models training history ones training has completed."]},{"cell_type":"code","metadata":{"id":"c5FAMBIyHtCV"},"source":["from itertools import cycle\n","def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 1.0], figsize=(6,6)):\n","  plt.figure(figsize=figsize)\n","  cycol = cycle('bgrcmk')\n","  for name, item in history_hold.items():\n","    y_train = item.history[metric]\n","    y_val = item.history['val_' + metric]\n","    x_train = np.arange(0,len(y_val))\n","\n","    c=next(cycol)\n","\n","    plt.plot(x_train, y_train, c+'-', label=name+'_train')\n","    plt.plot(x_train, y_val, c+'--', label=name+'_val')\n","\n","  plt.legend()\n","  plt.xlim([1, max(plt.xlim())])\n","  plt.ylim(ylim)\n","  plt.xlabel('Epoch')\n","  plt.ylabel(metric)\n","  plt.grid(True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"muMRu_4Xl2l2"},"source":["## Dataset \n","\n","CIFAR is an acronym that stands for the Canadian Institute For Advanced Research and the CIFAR-10 dataset was developed along with the CIFAR-100 dataset by researchers at the CIFAR institute.\n","\n","The dataset is comprised of 60,000 32×32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. The class labels and their standard associated integer values are listed below.\n","\n","- 0: airplane\n","- 1: automobile\n","- 2: bird\n","- 3: cat\n","- 4: deer\n","- 5: dog\n","- 6: frog\n","- 7: horse\n","- 8: ship\n","- 9: truck\n","\n","These are very small images, much smaller than a typical photograph, and the dataset was intended for computer vision research.\n","\n","CIFAR-10 is a well-understood dataset and widely used for benchmarking computer vision algorithms in the field of machine learning. The problem is “solved.” It is relatively straightforward to achieve 80% classification accuracy. Top performance on the problem is achieved by deep learning convolutional neural networks with a classification accuracy above 90% on the test dataset.\n","\n","- Performance metric: Accuracy\n","- Target performance: 90% Accuracy\n","\n","**Discuss why the above two selections are appropriate for the problem**"]},{"cell_type":"markdown","metadata":{"id":"b2CsFHQK7Ty7"},"source":["### Loading the dataset and Data Exploration\n","\n","First load the CIFAR-10 dataset using the dataset API."]},{"cell_type":"code","metadata":{"id":"qBY7L9sVlvW5"},"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","\n","dataset, info = tfds.load('cifar10', as_supervised = True, with_info = True)\n","test_dataset, train_dataset = dataset['test'], dataset['train']\n","\n","num_train_examples = info.splits['train'].num_examples\n","num_test_examples = info.splits['test'].num_examples\n","\n","print(num_train_examples, num_test_examples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjP6q9LRH8-U"},"source":["Plot some images"]},{"cell_type":"code","metadata":{"id":"oCyKI7rOwALV"},"source":["class_names = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n","plt.figure(figsize=(10,5))\n","i=1\n","for image,label in test_dataset.shuffle(100).take(10):\n","  plt.subplot(2,5,i)\n","  plt.imshow(image)\n","  plt.title(class_names[label.numpy()])\n","  i=i+1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dymJ9bRoIOhP"},"source":["Plot the histogram for labels. Checking class imbalance."]},{"cell_type":"code","metadata":{"id":"rOsN-7mA7zUg"},"source":["lab_hold = list()\n","for img, lab in test_dataset.take(num_train_examples):\n","  lab_hold.append(lab)\n","\n","plt.hist(lab_hold, bins=np.arange(0,11), rwidth=.5,align='left')\n","plt.ylabel('Number of Images')\n","plt.xlabel('Class Label')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuyM4gQqH8DV"},"source":["**Discuss what other forms of analysis is required to form your model. May need to revisit this section ones you started the developemnt**"]},{"cell_type":"markdown","metadata":{"id":"4cRVaSPNNq0u"},"source":["### Setup Data Loaders\n","\n","We will setup a data loader using Dataset API. You can also use keras datagenerator for this stage. **Use the knowladge from last week and pick the data loading mechanizm that suits best for your style of coding.**\n","\n","We are going to write our own augmentation functions. Unfortunately tensorflow does not provide a random rotation function under the `image` class. Therefore we are going to use the tensorflow addons to do this."]},{"cell_type":"code","metadata":{"id":"30b9Jt-OPiRh"},"source":["!pip install tfa-nightly"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QR02Z31-cinf"},"source":["Data Augmentation funcion"]},{"cell_type":"code","metadata":{"id":"3jOQ_FRjuGjf"},"source":["import tensorflow_addons as tfa\n","import numpy as np \n","\n","@tf.function\n","def rotate_tf(image,ang_deg=15):\n","    random_angles = tf.random.uniform(shape = (), minval = -np.deg2rad(ang_deg), maxval = np.deg2rad(ang_deg))\n","    return tfa.image.rotate(image,random_angles)\n","\n","@tf.function\n","def convert(image, label):\n","  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]\n","  return image, label\n","\n","@tf.function\n","def augment(image,label):\n","  image,label = convert(image, label)\n","  image = tf.image.resize_with_crop_or_pad(image, 40, 40) # Add 8 pixels of padding\n","  image = rotate_tf(image, 10) # Rorate in the range [0,10]\n","  image = tf.image.random_crop(image, size=[32, 32, 3]) # Random crop back to 32x23\n","  image = tf.image.random_flip_left_right(image)\n","  image = tf.image.random_brightness(image, max_delta=0.1) # Random brightness\n","\n","  return image,label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bvVEcCocnF0"},"source":["**What are the other types of data augmentation you can use?**\n","\n","Generate training (with and without augmetnation) and validation batches."]},{"cell_type":"code","metadata":{"id":"Tm-1egwGuNMv"},"source":["BATCH_SIZE=128\n","\n","# Set aside some train data as validation set to do hyperparameter tuning\n","num_val_examples = 10000\n","num_train_examples = num_train_examples - num_val_examples\n","\n","# Train set no augmetation\n","train_batches = (\n","    train_dataset\n","    .skip(num_val_examples)\n","    .take(num_train_examples)\n","    .cache()\n","    .shuffle(num_train_examples)\n","    .repeat()\n","    .map(convert, num_parallel_calls=AUTOTUNE)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTOTUNE)\n",") \n","\n","\n","# validation set no augmetation\n","val_batches = (\n","    train_dataset\n","    .take(num_val_examples)\n","    .cache()\n","    .map(convert, num_parallel_calls=AUTOTUNE)\n","    .batch(BATCH_SIZE)\n",") \n","\n","# Train set with augmetation\n","augmented_train_batches = (\n","    train_dataset\n","    .skip(num_val_examples)\n","    .take(num_train_examples)\n","    .cache()\n","    .shuffle(num_train_examples)\n","    .repeat()\n","    .map(augment, num_parallel_calls=AUTOTUNE)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTOTUNE)\n",") \n","\n","# Train set with augmetation\n","full_augmented_train_batches = (\n","    train_dataset\n","    .take(num_train_examples+num_val_examples)\n","    .cache()\n","    .shuffle(num_train_examples)\n","    .repeat()\n","    .map(augment, num_parallel_calls=AUTOTUNE)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTOTUNE)\n",") \n","\n","# Test set no augmetation: final evaluation\n","test_batches = (\n","    test_dataset\n","    .take(num_test_examples)\n","    .cache()\n","    .map(convert, num_parallel_calls=AUTOTUNE)\n","    .batch(BATCH_SIZE)\n",")\n","\n","# Tiny train set for debugginh only with 128 images\n","train_batches_tiny = (\n","    train_dataset\n","    .skip(num_val_examples)\n","    .take(128)\n","    .cache()\n","    .shuffle(num_train_examples)\n","    .repeat()\n","    .map(convert, num_parallel_calls=AUTOTUNE)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTOTUNE)\n",") "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hpO2RboF9StD"},"source":["Discuss the following:\n","- **What is the perpose of the function `cache()`?**\n","- **What will happen if `repeat` was not used?**\n","- **What id the function of `preferch`?**\n","\n","Lets take few images from augmented data stream and check if the output is as expected."]},{"cell_type":"code","metadata":{"id":"AqBOYDVYNqQL"},"source":["plt.figure(figsize=(10,5))\n","i=1\n","for image,label in augmented_train_batches.shuffle(100).take(10):\n","  plt.subplot(2,5,i)\n","  plt.imshow(image[0,:])\n","  plt.title(class_names[label[0].numpy()])\n","  i=i+1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mircehKLQcxE"},"source":["## (Optional) Writing custom CNN layer\n","\n","**This stage is not strictly necessary. You can do the model by just using keras sequential API. The code below is advance code which will make the final code more readable.**\n","\n","<font color='red'>**TODO:** If you are not using this section you need to write the function `get_resnet_model` yourself using the knowladge from labs week 2-5</font>\n","\n","\n","When building networks like ResNet, GoogleLeNet and VGG, we tend to repeat NN block with the same structure over and over again to build a deep model. \n","\n","While we can use the sequential/functional API with the existing blocks in tensorflow/keras to build such a large model, the code will become unreadble when the network size increases. \n","\n","A solution to this is to create custom layer for a local structure that can then be reapeated.\n","\n","Below we are going to create such a local block. This block is the resudial block from ResNet.\n","\n"]},{"cell_type":"code","metadata":{"id":"_QjrGNF4bOcQ"},"source":["class ResidualBlock(tf.keras.layers.Layer):\n","\n","    # Initialize components of the model\n","    def __init__(self, filter_num, stride=1, reg_lambda=0.0):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n","                                            kernel_size=(3, 3),\n","                                            strides=stride,\n","                                            kernel_initializer=\"he_normal\",\n","                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n","                                            padding=\"same\")\n","        self.bn1 = tf.keras.layers.BatchNormalization(momentum=.4)\n","        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n","                                            kernel_size=(3, 3),\n","                                            strides=1,\n","                                            kernel_initializer=\"he_normal\",\n","                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n","                                            padding=\"same\")\n","        self.bn2 = tf.keras.layers.BatchNormalization(momentum=.4)\n","        if stride != 1:\n","            self.downsample = tf.keras.Sequential()\n","            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n","                                                       kernel_size=(1, 1),\n","                                                       kernel_initializer=\"he_normal\",\n","                                                       kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n","                                                       strides=stride))\n","            self.downsample.add(tf.keras.layers.BatchNormalization(momentum=.4))\n","        else:\n","            self.downsample = lambda x: x\n","\n","    # Define the forward function\n","    def call(self, inputs, training=None, **kwargs):\n","        residual = self.downsample(inputs)\n","\n","        x = self.conv1(inputs)\n","        x = self.bn1(x, training=training)\n","        x = tf.nn.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x, training=training)\n","\n","        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n","\n","        return output\n","\n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'conv1': self.conv1,\n","            'bn1': self.bn1,\n","            'conv2': self.conv2,\n","            'bn2': self.bn2,\n","            'downsample': self.downsample,\n","        })\n","        return config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RSB0gQekR7Qu"},"source":["<font color='red'>**TODO:** Try to develop the bottle-neck residual block yourself later</font>"]},{"cell_type":"markdown","metadata":{"id":"9fJcl00yPjEo"},"source":["## Setting up the Models & attaching instrumentation\n","Write a function to generate a model with desired number of residual blocks."]},{"cell_type":"code","metadata":{"id":"2-F-s716NVZ8"},"source":["def get_resnet_model(filters, block_size, reg_lambda=0.0, fdropout=False):\n","  model = tf.keras.Sequential()\n","\n","  #initial segment\n","  model.add(tf.keras.layers.Conv2D(filters=64,\n","                                   kernel_size=(3, 3),\n","                                   strides=1,\n","                                   kernel_initializer=\"he_normal\",\n","                                   kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n","                                   padding=\"same\", input_shape=(32, 32, 3)))\n","  model.add(tf.keras.layers.BatchNormalization(momentum=.4))\n","\n","  #Stack of residual blocks\n","  for nFilters, nBlocks in zip(filters, block_size):\n","    model.add(ResidualBlock(nFilters, stride=2, reg_lambda=reg_lambda))\n","    \n","    for _ in range(1, nBlocks):\n","      model.add(ResidualBlock(nFilters, stride=1, reg_lambda=reg_lambda))\n","\n","  # Final part\n","  model.add(tf.keras.layers.GlobalAveragePooling2D())\n","  model.add(tf.keras.layers.Flatten())\n","  model.add(tf.keras.layers.Dense(10, \n","                                  activation=tf.nn.softmax, \n","                                  kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n","                                  kernel_initializer=\"he_normal\"))\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G37LNeYAtuMd"},"source":["Define the callabcks and optimizer"]},{"cell_type":"code","metadata":{"id":"nDZ2FyEQtvpO"},"source":["from tensorflow.keras.optimizers import Adam, SGD\n","\n","epochs = 100\n","\n","STEPS_PER_EPOCH = num_train_examples//BATCH_SIZE\n","lr = 0.001\n","\n","lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n","  lr,\n","  decay_steps=STEPS_PER_EPOCH*1000,\n","  decay_rate=10,\n","  staircase=False)\n","\n","optimizer = Adam(learning_rate=lr_schedule)\n","# lr = 0.01\n","# optimizer = SGD(learning_rate=lr, momentum=0.9)\n","\n","m_histories = {}\n","def get_callbacks(name, early_stop=True):\n","  if early_stop:\n","    return [\n","            tf.keras.callbacks.EarlyStopping(monitor='val_SparseCategoricalCrossentropy', patience=25),\n","            tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=60, embeddings_freq=60),\n","            ]\n","  else:\n","    return [tf.keras.callbacks.TensorBoard(logdir/name)]\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Se2rT_CNdgXS"},"source":["Plot the lerning rate policy"]},{"cell_type":"code","metadata":{"id":"z_t_qB4QXg0s"},"source":["step = np.linspace(0,100000)\n","lr = lr_schedule(step)\n","plt.figure(figsize = (8,6))\n","plt.plot(step/STEPS_PER_EPOCH, lr)\n","plt.ylim([0,max(plt.ylim())])\n","plt.xlabel('Epoch')\n","_ = plt.ylabel('Learning Rate')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B0u8Wsbl7rFm"},"source":["## Developing the Models\n","\n","Now lets start to develop the model. "]},{"cell_type":"markdown","metadata":{"id":"W6s2uhFi_Cvp"},"source":["### Tiny model\n","\n","Lets do a very small model with just one resudial block. It is highly probable that this model will underfit. However it is good to do a sanitiy check with a simple model to see if the code we developed so far is working.\n","\n","Get a model, compile and train it."]},{"cell_type":"code","metadata":{"id":"pQ7p1P8wxWdD"},"source":["tiny_res_net = get_resnet_model([64,], [1,])\n","tiny_res_net.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","              metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","m_histories['resnet_tiny'] = tiny_res_net.fit(train_batches, \n","                                              epochs=epochs, \n","                                              steps_per_epoch=num_train_examples//BATCH_SIZE, \n","                                              validation_data=val_batches,\n","                                              validation_steps=num_val_examples//BATCH_SIZE, \n","                                              verbose=0, \n","                                              callbacks=get_callbacks('models/resnet_tiny') )\n","\n","plotter(m_histories, ylim=[0.0, 2.5], metric = 'SparseCategoricalCrossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Mih5yBF6Ijb"},"source":["### Tiny Dataset\n","\n","Check if the model overfits to a tiny dataset to see if there is any coding errors. "]},{"cell_type":"code","metadata":{"id":"Inir3xX76OkL"},"source":["tiny_res_net2 = get_resnet_model([64,], [1,])\n","tiny_res_net2.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","              metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","m_histories['resnet_tiny_dataset'] = tiny_res_net2.fit(train_batches_tiny, \n","                                              epochs=epochs, \n","                                              steps_per_epoch=num_train_examples//BATCH_SIZE, \n","                                              validation_data=val_batches,\n","                                              validation_steps=num_val_examples//BATCH_SIZE, \n","                                              verbose=0, \n","                                              callbacks=get_callbacks('models/resnet_tiny_dataset') )\n","\n","plotter(m_histories, ylim=[0.0, 5.0], metric = 'SparseCategoricalCrossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Av_HBcB7__1G"},"source":["**What do you observe? high bias or high varience?**\n","\n","**What is the solutions that can be tried if you model is showing high bias (underfitting)?**"]},{"cell_type":"markdown","metadata":{"id":"BNdkT-3Y-5Xc"},"source":["### Large Model (Base Model)\n","\n","A solution for high bias is to increase the capacity of the model. This can be done by adding more depth (mode residual blocks). \n","\n","Lets now do a large mode with three groups of residual blocks each with 3 residual blocks.\n","\n","Get a model, compile and train it."]},{"cell_type":"code","metadata":{"id":"WdUxXCFUTPWY"},"source":["b_histories = {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SlFGTmEg1BIJ"},"source":["large_res_net = get_resnet_model([64, 128, 256], [3, 3, 3])\n","large_res_net.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","              metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","b_histories['resnet_large'] = large_res_net.fit(train_batches, \n","                                                epochs=epochs, \n","                                                steps_per_epoch=num_train_examples//BATCH_SIZE, \n","                                                validation_data=val_batches,\n","                                                validation_steps=num_val_examples//BATCH_SIZE, \n","                                                verbose=0, \n","                                                callbacks=get_callbacks('models/resnet_large') )\n","\n","plotter(b_histories, ylim=[0.0, 2.5], metric = 'SparseCategoricalCrossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDOhlbazA33a"},"source":["- **What do you observe now - high bias/high varience?**\n","- **What are the options avaialbe when the model is showing high varience?**"]},{"cell_type":"markdown","metadata":{"id":"-a3rhMMP_fOv"},"source":["### Data Augmentation\n","\n","There are several options when the model is having high varince:\n","\n","- Get more data (not plausiable)\n","- Data Augmentation.\n","- Regularization: weight penelty\n","- Reduce model capacity by changing the structure.\n","\n","Lets first start with data augmentation. Here we will train the previous model with data augmentation. \n"]},{"cell_type":"code","metadata":{"id":"sQpa5i9w1TOS"},"source":["large_res_net_aug = get_resnet_model([64, 128, 256], [3, 3, 3])\n","large_res_net_aug.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","              metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","b_histories['resnet_large_aug'] = large_res_net_aug.fit(augmented_train_batches, \n","                                                        epochs=epochs, \n","                                                        steps_per_epoch=num_train_examples//BATCH_SIZE, \n","                                                        validation_data=val_batches,\n","                                                        validation_steps=num_val_examples//BATCH_SIZE, \n","                                                        verbose=0, \n","                                                        callbacks=get_callbacks('models/resnet_large_aug') )\n","\n","plotter(b_histories, ylim=[0.0, 2.5], metric = 'SparseCategoricalCrossentropy', figsize=(10,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zKNI7UXVTt0W"},"source":["**What do you observe: High bias/ High Varience?**"]},{"cell_type":"markdown","metadata":{"id":"hVPegzqbCJNO"},"source":["### Regularization\n","\n","Lets apply some more techniques to reduce high varience."]},{"cell_type":"code","metadata":{"id":"oVasq21L1o09"},"source":["large_res_net_reg = get_resnet_model([64, 128, 256], [3, 3, 3], reg_lambda=0.001)\n","large_res_net_reg.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","              metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","b_histories['resnet_large_reg'] = large_res_net_reg.fit(augmented_train_batches, \n","                                                        epochs=epochs, \n","                                                        steps_per_epoch=num_train_examples//BATCH_SIZE, \n","                                                        validation_data=val_batches,\n","                                                        validation_steps=num_val_examples//BATCH_SIZE, \n","                                                        verbose=0, \n","                                                        callbacks=get_callbacks('models/resnet_large_reg') )\n","\n","plotter(b_histories, ylim=[0.0, 2.5], metric = 'SparseCategoricalCrossentropy', figsize=(10,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwUdtjlpUoNc"},"source":["<font color='red'>**TODO:** Select the best value for the hyper parameter lambda </font>\n"]},{"cell_type":"code","metadata":{"id":"dmhlC_TZVjd9"},"source":["h_histories = {}\n","lambda_vals = [0.05, 0.01, 0.005, 0.001]\n","# lambda_vals = [0.005, 0.002, 0.001,  0.0005, 0.0002, 0.0001]\n","\n","for reg_lambda in lambda_vals:\n","  large_res_net_reg_ = get_resnet_model([64, 128, 256], [3, 3, 3], reg_lambda=reg_lambda)\n","  large_res_net_reg_.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","                metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","  h_histories['resnet_large_reg'+ '_h' + str(reg_lambda)] = large_res_net_reg_.fit(augmented_train_batches, \n","                                                          epochs=epochs, \n","                                                          steps_per_epoch=num_train_examples//BATCH_SIZE, \n","                                                          validation_data=val_batches,\n","                                                          validation_steps=num_val_examples//BATCH_SIZE, \n","                                                          verbose=0, \n","                                                          callbacks=get_callbacks('models/resnet_large_reg'+ '_h1' + str(reg_lambda), early_stop=True) )\n","  \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v6m78FgpcuAJ"},"source":["plt.figure(figsize=(10,5))\n","metric = 'SparseCategoricalCrossentropy'\n","l_train = list()\n","l_val = list()\n","\n","for reg_lambda in lambda_vals:\n","  l_train.append(h_histories['resnet_large_reg'+ '_h' + str(reg_lambda)].history[metric][-1])\n","  l_val.append(h_histories['resnet_large_reg'+ '_h' + str(reg_lambda)].history['val_' + metric][-1])\n","\n","plt.plot(lambda_vals,l_train, 'ro', label='Train' )\n","plt.plot(lambda_vals,l_val, 'bs', label='Test' )\n","\n","plt.xlabel('Lambda', fontsize=14)\n","plt.ylabel('Categorical Crossentropy', fontsize=14)\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RKOqlrR8Vhb3"},"source":["\n","**Have you achived the target metric value? What options would you like to try?**"]},{"cell_type":"markdown","metadata":{"id":"vVf_0mVDIp_c"},"source":["### Huge Model"]},{"cell_type":"code","metadata":{"id":"ONXiCBeLiOuY"},"source":["huge_res_net_reg = get_resnet_model([64, 128, 256, 512], [6, 6, 6, 6], reg_lambda=0.005 )\n","# large_res_net_reg2.summary()\n","huge_res_net_reg.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","              metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","b_histories['resnet_huge_reg'] = huge_res_net_reg.fit(augmented_train_batches, \n","                                                        epochs=epochs, \n","                                                        steps_per_epoch=num_train_examples//BATCH_SIZE, \n","                                                        validation_data=val_batches,\n","                                                        validation_steps=num_val_examples//BATCH_SIZE, \n","                                                        verbose=0, \n","                                                        callbacks=get_callbacks('models/resnet_huge_reg') )\n","\n","plotter(b_histories, ylim=[0.0, 2.5], metric = 'SparseCategoricalCrossentropy', figsize=(10,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q_bsMECQXrQ_"},"source":["<font color='red'>**TODO:** Select the best value for the hyper parameter lambda </font>\n","\n","<font color='red'>**TODO:** Train the best model you obtained using the entire training dataset (selected hyperparameters) </font>"]},{"cell_type":"code","metadata":{"id":"B7UMfeGlnWNt"},"source":["final_model = get_resnet_model([64, 128, 256], [3, 3, 3], reg_lambda=0.005)\n","final_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\",\n","              metrics=[tf.losses.SparseCategoricalCrossentropy(from_logits=False, name='SparseCategoricalCrossentropy'), 'accuracy'])\n","\n","b_histories['final_model'] = final_model.fit(full_augmented_train_batches, \n","                                                        epochs=epochs, \n","                                                        steps_per_epoch=(num_train_examples+num_val_examples)//BATCH_SIZE, \n","                                                        validation_data=test_batches,\n","                                                        validation_steps=num_test_examples//BATCH_SIZE, \n","                                                        verbose=0, \n","                                                        callbacks=get_callbacks('models/final_model') )\n","\n","plotter(b_histories, ylim=[0.0, 2.5], metric = 'SparseCategoricalCrossentropy', figsize=(10,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w0WFvJBjIu8r"},"source":["### Testing models "]},{"cell_type":"code","metadata":{"id":"jIsyOhJqX4yX"},"source":["print(tiny_res_net.evaluate(test_batches))\n","print(large_res_net.evaluate(test_batches))\n","print(large_res_net_aug.evaluate(test_batches))\n","print(large_res_net_reg.evaluate(test_batches))\n","print(huge_res_net_reg.evaluate(test_batches))\n","print(final_model.evaluate(test_batches))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcZPaiRc0x3f"},"source":["**Have you achived the desired performance?**  \n","**If not, What options would you want to consider?**\n"]},{"cell_type":"markdown","metadata":{"id":"FGV2ja8YDCQZ"},"source":["### Plotting results\n","We would like to plot some results on the test(validation) set to see if the model is performing reasonably. \n","\n","This analysis can be used to identify components of the model that need to be improved. "]},{"cell_type":"code","metadata":{"id":"CarJgqZiuoCl"},"source":["plt.figure(figsize=(20,7))\n","\n","for image,label in test_batches.shuffle(100).take(1):\n","  pred_y = final_model.predict(image)\n","  pred_y_ = np.argmax(pred_y, axis=1)\n","  for i in range(8):\n","    plt.subplot(1,8,i+1)\n","    plt.imshow(image[i,:])\n","    plt.title(class_names[label[i].numpy()] + ' -> ' + class_names[pred_y_[i]] )\n","\n","  plt.figure(figsize=(20,3))\n","  for i in range(8):\n","    plt.subplot(1,8,i+1)\n","    plt.bar(np.arange(0,10), np.squeeze(pred_y[i,:]))\n","    plt.xticks(np.arange(0,10), ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'],  rotation='vertical')\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRzpiDhup3Ln"},"source":["plt.figure(figsize=(20,7))\n","\n","for image,label in test_batches.shuffle(100).take(1):\n","  pred_y = final_model.predict(image)\n","  pred_y_ = np.argmax(pred_y, axis=1)\n","  for i in range(8):\n","    plt.subplot(1,8,i+1)\n","    plt.imshow(image[i,:])\n","    plt.title(class_names[label[i].numpy()] + ' -> ' + class_names[pred_y_[i]] )\n","\n","  plt.figure(figsize=(20,3))\n","  for i in range(8):\n","    plt.subplot(1,8,i+1)\n","    plt.bar(np.arange(0,10), np.squeeze(pred_y[i,:]))\n","    plt.xticks(np.arange(0,10), ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'],  rotation='vertical')\n","    "],"execution_count":null,"outputs":[]}]}