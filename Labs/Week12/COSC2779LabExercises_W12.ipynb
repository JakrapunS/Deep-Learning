{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COSC2779LabExercises_W12.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOYlkrYqgb67IVgce2Gjk0v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-uh21p1YP2Hb"},"source":["---\n","# <div align=\"center\"><font color='blue'>  </font></div>\n","# <div align=\"center\"><font color='blue'> COSC 2779 | Deep Learning  </font></div>\n","## <div align=\"center\"> <font color='blue'> Week 12 Lab Exercises: **Model Interpretation**</font></div>\n","---"]},{"cell_type":"markdown","metadata":{"id":"0mL02YhQkkoe"},"source":["# Introduction\n","\n","In this lab, you will learn some basic techniques for deep neural network model interpretation. \n","\n","In this lab, you will:\n","- Use self-developed scripts to do\n","  - Feature Visualisation\n","  - Feature Attribution\n","- Use functionality in `tf-explain` library to do model interpretation.\n","\n","![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)  This notebook is designed to run on Google Colab. If you like to run this on your local machine, make sure that you have installed TensorFlow version 2.0. "]},{"cell_type":"markdown","metadata":{"id":"oklfH-2J_cSH"},"source":["## Setting up the Notebook\n","\n","Let's first load the packages we need."]},{"cell_type":"code","metadata":{"id":"_wbRo7f4QsMP"},"source":["import tensorflow as tf\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow_datasets as tfds\n","import pathlib\n","import shutil\n","import tempfile\n","\n","from  IPython import display\n","from matplotlib import pyplot as plt\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import layers, losses\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1TVJBjn6zYa"},"source":["## Setting up Instrumentation\n","\n","We can use the tensor board to view the learning curves, activation and weight hostograms. Lets first set it up."]},{"cell_type":"code","metadata":{"id":"rN5-iJ44HhrP"},"source":["logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n","shutil.rmtree(logdir, ignore_errors=True)\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Open an embedded TensorBoard viewer\n","%tensorboard --logdir {logdir}/models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOM74Hd1lBr3"},"source":["## Setting up the model\n","\n","First we will read an image that we will be using for this lab. "]},{"cell_type":"code","metadata":{"id":"X4hXOjq6kHRM"},"source":["from google.colab import files\n","upload_file = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wnY56MHAkePk"},"source":["IMAGE_PATH = './' + list(upload_file.keys())[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tj6KOWeqlP5L"},"source":["In this lab we will try to interpret the ResNet50V2 model trained on ImageNet Task. First we need to get the model from keras applications and load the weights. "]},{"cell_type":"code","metadata":{"id":"7rVKy0-_nq2X"},"source":["# Model to examine\n","model = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=True)\n","\n","from keras.utils.vis_utils import plot_model\n","# plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BXVSBtiRcuy"},"source":["plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBhaqFYjmIfE"},"source":["Now lets predict the class for the and print the results. "]},{"cell_type":"code","metadata":{"id":"zmfQVYHRr4Kr"},"source":["# Preprocess the Image to pass as input\n","img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n","img = tf.keras.preprocessing.image.img_to_array(img)\n","img_input = tf.expand_dims(img, axis=0)\n","img_input = tf.cast(img_input, tf.float32)\n","img_input = tf.keras.applications.resnet_v2.preprocess_input(img_input)\n","\n","# Do the prediction\n","prediction = model.predict(img_input)\n","\n","# Get the top 5 predictions and print\n","top5_pred = tf.keras.applications.imagenet_utils.decode_predictions(prediction, top=5)\n","\n","for class_id, class_name , prob in top5_pred[0]:\n","  print('{} : {:2.2f}'.format(class_name, prob))\n","\n","plt.imshow((img).astype(np.int))\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sl4H5jGFRDal"},"source":["# Model interpritation with self-developed scripts\n","\n","Lets write some python scripts to do model interpritation. This section is designed to gove you a brief overview of the underlying tehniques. "]},{"cell_type":"markdown","metadata":{"id":"pj8-m639mTSq"},"source":["## Feature Visualisation\n","\n","In this segment we will try out some feature visualisation techniques"]},{"cell_type":"markdown","metadata":{"id":"v4ZkhPYomibR"},"source":["### Plot learned weights of the first layer"]},{"cell_type":"code","metadata":{"id":"PqQtYMhUY7UO"},"source":["weights = model.get_layer('conv1_conv').get_weights()[0]\n","print(weights.shape)\n","\n","plt.figure(figsize=(10,10))\n","\n","for i in range(0, weights.shape[-1]):\n","  weights_ = weights[:,:,:,i]\n","  weights_ = (weights_-weights_.min())/(weights_.max()-weights_.min())\n","  plt.subplot(8,8,i+1)\n","  plt.imshow(weights_)\n","  plt.axis('off')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2iSSrAlRn44h"},"source":["### Output visualisation of a layer\n","\n","For middle layers in a CNN we can simply visualise what comes out of the activation layers. Does the output still look relevant? Or does it look like random noise? By examining how the image transits through the network, you can validate that it focuses on the right regions."]},{"cell_type":"code","metadata":{"id":"OUqksOY8jk7b"},"source":["layers_name = ['conv2_block2_2_relu']\n","\n","# Get the outputs of layers we want to inspect\n","outputs = [\n","    layer.output for layer in model.layers\n","    if layer.name in layers_name\n","]\n","\n","# Create a connection between the input and those target outputs\n","activations_model = tf.keras.models.Model(model.inputs, outputs=outputs)\n","activations_model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Get their outputs\n","activations_1 = activations_model.predict(img_input)\n","\n","print('Activation Shape: ', activations_1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGYoSyzQmn8h"},"source":["plt.figure(figsize=(20,20))\n","for i in range(1,64+1):\n","  plt.subplot(8,8,i)\n","  plt.imshow(activations_1[0,:,:,i-1])\n","  plt.axis('off')\n","  \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pf2PJbthBilw"},"source":["### Maximal Activation Input\n","\n","Seeing what is coming out of a layer is great, but what if we could understand what makes a kernel activate?\n","\n","Here we want to generate an input to the network that maximises the output of a given filter. Therefore, we create a  new sub model that goes from the input layer to the layer we are interested in. The loss function we seek to maximise is the mean of this activation layerâ€™s output. \n","\n","We start from a random noise image and then update the image pixels using backprop of the loss defined above. Note that here we do not update the weights of the CNN."]},{"cell_type":"code","metadata":{"id":"wfosNCr3Ji__"},"source":["# Layer name to inspect\n","layer_name = 'conv5_block2_out'\n","\n","# Create a connection between the input and the target layer\n","submodel = tf.keras.models.Model(model.inputs, model.get_layer(layer_name).output)\n","\n","epochs = 100\n","step_size = 0.1\n","filter_index = 25\n","\n","# Initiate random noise\n","input_img_data = np.random.random((1, 224, 224, 3))\n","input_img_data = (input_img_data - 0.5) * 20 + 128.\n","input_img_data = tf.cast(input_img_data, tf.float32)\n","input_img_data = tf.keras.applications.resnet_v2.preprocess_input(input_img_data)\n","\n","input_img_data = tf.Variable(input_img_data)\n","\n","# Iterate gradient ascents\n","for _ in range(epochs):\n","    with tf.GradientTape() as tape:\n","        outputs = submodel(input_img_data)\n","        loss_value = tf.reduce_mean(outputs[:,:,:, filter_index]) + 0.01 * tf.reduce_mean(input_img_data**2)\n","    grads = tape.gradient(loss_value, input_img_data)\n","    normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)\n","    input_img_data.assign_add(normalized_grads * step_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gqPQDIAJjAE"},"source":["resmap = input_img_data[0,:,:,:].numpy()\n","resmap = (resmap - resmap.min()) / (resmap.max() - resmap.min())\n","plt.figure(figsize=(10,10))\n","plt.imshow((resmap*255).astype(np.int))\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RKw3TccBDD08"},"source":["## Feature Attribution\n","\n","In this section we are interested in knowing How did each input contribute to a particular prediction.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RxxGuja5EEDm"},"source":["### Saliency via occlusion\n","For this we are first going to use Saliency via occlusion. \n","\n","In this method, we Mask part of the image before it is fed to the CNN. See how much the prediction probability for a class change."]},{"cell_type":"code","metadata":{"id":"BAAmQNfLzgJw"},"source":["# Create function to apply a grey patch on an image\n","def apply_grey_patch(image, top_left_x, top_left_y, patch_size):\n","    patched_image = np.array(image, copy=True)\n","    patched_image[top_left_y:top_left_y + patch_size, top_left_x:top_left_x + patch_size, :] = 127.5\n","\n","    return patched_image\n","\n","# Load image\n","img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n","img = tf.keras.preprocessing.image.img_to_array(img)\n","\n","# category to id mapping in ImageNet\n","# German_shepherd -> 235\n","# tennis_ball -> 852\n","# Walker_hound -> 166\n","# Tabby_cat -> 281\n","\n","\n","CLASS_INDEX = 235  # Imagenet class index\n","PATCH_SIZE = 60\n","\n","sensitivity_map = np.zeros((img.shape[0], img.shape[1]))\n","count_map = np.zeros((img.shape[0], img.shape[1]))\n","\n","# Iterate the patch over the image\n","for top_left_x in range(0, img.shape[0], PATCH_SIZE//8):\n","    for top_left_y in range(0, img.shape[1], PATCH_SIZE//8):\n","        patched_image = apply_grey_patch(img, top_left_x, top_left_y, PATCH_SIZE)\n","\n","        patched_image = tf.expand_dims(patched_image, axis=0)\n","        patched_image = tf.cast(patched_image, tf.float32)\n","        patched_image = tf.keras.applications.resnet_v2.preprocess_input(patched_image)\n","\n","        predicted_classes = model.predict(patched_image)[0]\n","        confidence = predicted_classes[CLASS_INDEX]\n","        \n","        # Save confidence for this specific patched image in map\n","        sensitivity_map[\n","            top_left_y:top_left_y + PATCH_SIZE,\n","            top_left_x:top_left_x + PATCH_SIZE,\n","        ] = sensitivity_map[top_left_y:top_left_y + PATCH_SIZE,top_left_x:top_left_x + PATCH_SIZE,] + confidence\n","\n","        count_map[\n","            top_left_y:top_left_y + PATCH_SIZE,\n","            top_left_x:top_left_x + PATCH_SIZE,\n","        ] = count_map[top_left_y:top_left_y + PATCH_SIZE,top_left_x:top_left_x + PATCH_SIZE,] + 1.0\n","\n","sensitivity_map = sensitivity_map / count_map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PS4PP2pPD-pq"},"source":["plot the sensitivity map."]},{"cell_type":"code","metadata":{"id":"u9b6riDR0_rW"},"source":["sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min())\n","sensitivity_map = (sensitivity_map -1 )*-1 # Invert color space\n","\n","plt.figure(figsize=(5,5))\n","plt.imshow(img.astype(np.int))\n","plt.imshow((sensitivity_map*255).astype(np.int), alpha=.5,cmap='jet')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TdQPaiFCEJJR"},"source":["### Saliency via Backprop\n","\n","Compute gradient of class score with respect to image pixels. Take absolute value and max over RGB channels."]},{"cell_type":"code","metadata":{"id":"iLncjVNdAQOZ"},"source":["import matplotlib.cm as cm\n","\n","CLASS_INDEX = 235\n","LAYER_NAME = 'predictions'\n","\n","# Read and pre process the image\n","img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n","img = tf.keras.preprocessing.image.img_to_array(img)\n","img_input = tf.expand_dims(img, axis=0)\n","img_input = tf.cast(img_input, tf.float32)\n","img_input = tf.keras.applications.resnet_v2.preprocess_input(img_input)\n","\n","# Create a sub model\n","img_input = tf.Variable(img_input)\n","grad_model_bp = tf.keras.models.Model([model.inputs], [model.get_layer(LAYER_NAME).output])\n","\n","# compute the gradinets\n","with tf.GradientTape() as tape:\n","    predictions = grad_model_bp(img_input)\n","    loss = predictions[:, CLASS_INDEX]\n","\n","grads = tape.gradient(loss, img_input)\n","\n","\n","#Normalize and plot\n","print(grads.numpy().shape)\n","grads = grads.numpy()[0,:,:,:]\n","grads = np.abs(grads)\n","grads = np.max(grads, axis=-1)\n","print(grads.shape)\n","\n","grads = (grads - grads.min()) / (grads.max() - grads.min())\n","plt.figure(figsize=(5,5))\n","plt.imshow(img.astype(np.int))\n","plt.imshow((grads*255).astype(np.int), alpha=.5,cmap='jet')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DINjX4RwEyFD"},"source":["### GRAD-CAM\n","\n","Grad-cam combines saliency by backprop and class activation maps. This method is mostly applicable to CNN architectures that use global avarage pooling "]},{"cell_type":"code","metadata":{"id":"XBt9fSLw4d0A"},"source":["import matplotlib.cm as cm\n","\n","LAYER_NAME = 'post_relu'\n","CLASS_INDEX = 235\n","\n","#read image and preprocess\n","img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n","img = tf.keras.preprocessing.image.img_to_array(img)\n","img_input = tf.expand_dims(img, axis=0)\n","img_input = tf.cast(img_input, tf.float32)\n","img_input = tf.keras.applications.resnet_v2.preprocess_input(img_input)\n","\n","# get a sub model to output final prediction and the output before GAP layer\n","grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(LAYER_NAME).output, model.output])\n","\n","# compute the gradients\n","with tf.GradientTape() as tape:\n","    conv_outputs, predictions = grad_model(img_input)\n","    loss = predictions[:, CLASS_INDEX]\n","\n","output = conv_outputs[0]\n","grads = tape.gradient(loss, conv_outputs)[0]\n","\n","# guided backprop\n","gate_f = tf.cast(output > 0, 'float32')\n","gate_r = tf.cast(grads > 0, 'float32')\n","guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n","\n","weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n","\n","# CAM\n","cam = np.ones(output.shape[0: 2], dtype = np.float32)\n","\n","for i, w in enumerate(weights):\n","    cam += w * output[:, :, i]\n","\n","\n","#Plot results\n","cam = cam.numpy()\n","cam = np.maximum(cam, 0)\n","heatmap = (cam - cam.min()) / (cam.max() - cam.min())\n","                                               \n","from PIL import Image\n","heatmap = Image.fromarray(np.uint8(heatmap*255))\n","heatmap = heatmap.resize((224, 224))\n","\n","plt.figure(figsize=(5,5))\n","plt.imshow(img.astype(np.int))\n","plt.imshow(heatmap, alpha=.5,cmap='jet')\n","plt.axis('off')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CB1vMuiUSV0W"},"source":["# Using `tf-explain` library for model interpritations\n","\n","Lets look at some examples of using tf-explain library"]},{"cell_type":"code","metadata":{"id":"e9Zwgo2GSgcB"},"source":["!pip install tf-explain"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fztJSFnQdKhn"},"source":["## SmoothGrad"]},{"cell_type":"markdown","metadata":{"id":"sINZtuP-cl8y"},"source":["Lets see why ResNet model classifiy the given image as german shepard."]},{"cell_type":"code","metadata":{"id":"oQ0XTspSSzJw"},"source":["from tf_explain.core.smoothgrad import SmoothGrad\n","\n","CLASS_INDEX = 235 #german sheprard\n","model = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=True)\n","\n","#read image and preprocess\n","img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n","img = tf.keras.preprocessing.image.img_to_array(img)\n","img = tf.keras.applications.resnet_v2.preprocess_input(img)\n","\n","data = ([img], None)\n","\n","explainer = SmoothGrad()\n","# Compute SmoothGrad on VGG16\n","grid = explainer.explain(data, model, CLASS_INDEX, 20, 1.,)\n","explainer.save(grid, '.', 'smoothgrad.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzWEwFmiTayw"},"source":["plt.figure(figsize=(5,5))\n","plt.imshow(((img/2.0+0.5)*255).astype(np.uint8))\n","plt.imshow(grid, alpha=.5,cmap='jet')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVnvsn-6cx4K"},"source":["Lets see why woud ResNet model give high probability for the given image as tennis ball."]},{"cell_type":"code","metadata":{"id":"2rKUecAjT8WO"},"source":["CLASS_INDEX = 852 # tennis ball\n","\n","grid = explainer.explain(data, model, CLASS_INDEX, 20, .05)\n","explainer.save(grid, '.', 'smoothgrad.png')\n","\n","plt.figure(figsize=(5,5))\n","plt.imshow(((img/2.0+0.5)*255).astype(np.uint8))\n","plt.imshow(grid, alpha=.5,cmap='jet')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plXTPUjXdN_k"},"source":["## GradCAM"]},{"cell_type":"code","metadata":{"id":"LoVhQekufhaV"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ao4mW9P7dQcO"},"source":["from tf_explain.core.grad_cam import GradCAM\n","CLASS_INDEX = 235 # tennis ball\n","\n","img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n","img = tf.keras.preprocessing.image.img_to_array(img)\n","img = tf.keras.applications.resnet_v2.preprocess_input(img)\n","\n","data = ([img], None)\n","\n","explainer = GradCAM()\n","grid = explainer.explain(data, model, class_index=CLASS_INDEX, layer_name=\"conv5_block3_3_conv\" )\n","explainer.save(grid, '.', 'smoothgrad.png')\n","\n","plt.figure(figsize=(5,5))\n","plt.imshow(((img/2.0+0.5)*255).astype(np.uint8))\n","plt.imshow(grid, alpha=.5,cmap='jet')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56UdXHi5gs0P"},"source":["CLASS_INDEX = 852 # tennis ball\n","grid = explainer.explain(data, model, class_index=CLASS_INDEX, layer_name=\"conv5_block3_3_conv\" )\n","explainer.save(grid, '.', 'smoothgrad.png')\n","\n","plt.figure(figsize=(5,5))\n","plt.imshow(((img/2.0+0.5)*255).astype(np.uint8))\n","plt.imshow(grid, alpha=.5,cmap='jet')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EdJLKqMkSLh"},"source":["More information on `tf-explain` is at [URL](https://tf-explain.readthedocs.io/en/latest/overview.html)"]}]}